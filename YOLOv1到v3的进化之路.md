## YOLOv1到v3的进化之路

相对于传统的分类问题，目标检测显然更符合现实需求，因为往往现实中不可能在某一个场景只有一个物体，因此目标检测的需求变得更为复杂，不仅仅要求算法能够检验出是什么物体，还需要确定这个物体在图片哪里。

在这一过程中，目标检测经历了一个高度的符合人类的直觉的过程。既需要识别出目标的位置，将图片划分成小图片扔进算法中去，当算法认为某物体在这个小区域上之时，那么检测完成。那我们就认为这个物体在这个小图片上了。而这个思路，正是比较早期的目标检测思路，比如R-CNN。

后来的Fast R-CNN，Faster R-CNN[16]虽有改进，比如不再是将图片一块块的传进CNN提取特征，而是整体放进CNN提取特征图后，再做进一步处理，但依旧是整体流程分为 ‘区域提取’和‘目标分类’两部分（two-stage），这样做的一个特点是虽然确保了精度，但速度非常慢，于是以YOLO（You only look once）为主要代表的这种一步到位(one-stage)即端到端的目标检测算法应运而生了。

### YOLO V1 一步检测的开山之作

#### 1. YOLO 的核心思想

YOLO 的核心思想就是利用整张图作为网络的输入，直接在输出层回归 bounding box（边界框） 的位置及其所属的类别。

#### 2. YOLO 的实现方法

![20180606164153997.png](/images/20180606164153997.png)

- 将一幅图像分成 SxS 个网格（grid cell），如果某个 object 的中心落在这个网格中，则这个网格就负责预测这个 object。

- 每个网格要预测 B 个 bounding box，每个 bounding box 除了要回归自身的位置之外，还要附带预测一个 confidence 值。

这个 confidence 代表了所预测的 box 中含有 object 的置信度和这个 box 预测的有多准这两重信息，其值是这样计算的： 
![20180606164218784.png](/images/20180606164218784.png)

这个置信度并不只是该边界框是待检测目标的概率，而是该边界框是**待检测目标的概率乘上该边界框和真实位置的IoU**（框之间的交集除以并集）的积。通过乘上这个交并比，反映出该边界框预测位置的精度。

每个边界框对应于5个输出，分别是x，y，w，h和置信度。其中x，y代表边界框的中心离开其所在网格单元格边界的偏移。w，h代表边界框真实宽高相对于整幅图像的比例。x，y，w，h这几个参数都已经被限制到了区间[0,1]上。除此以外，每个单元格还产生C( categories)个条件概率。

举例说明: 在 PASCAL VOC 中，图像输入为 448x448，取 S=7，B=2，一共有20 个类别（C=20），则输出就是 S x S x (5*B+C) = 7x7x30 的一个 tensor。

网络结构：

![20180606164310266.png](/images/20180606164310266.png)

在test的非极大值抑制阶段，每个网格预测的 class 信息和 bounding box 预测的 confidence信息相乘，就得到每个 bounding box 的 class-specific confidence score，即下式衡量该框是否应该予以保留。

![20180606164339450.png](/images/20180606164339450.png)

> 注：
由于输出层为全连接层，因此在检测时，YOLO 训练模型只支持与训练图像相同的输入分辨率。

> 虽然每个格子可以预测 B 个 bounding box，但是最终只选择只选择 IOU 最高的 bounding box 作为物体检测输出，即每个格子最多只预测出一个物体。当物体占画面比例较小，如图像中包含畜群或鸟群时，每个格子包含多个物体，但却只能检测出其中一个。这是 YOLO 方法的一个缺陷。





